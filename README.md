Emotion intensities provide insights into how strongly an emotion is expressed by a human. However, voice-based studies on emotion intensities have not been explored in sufficient depth. Most existing research categorizes emotions into binary categories, such as pleasant or unpleasant, without delving into finer nuances. Understanding emotional intensities based on the pleasantness of emotions could offer a baseline for determining the degree to which an emotion is expressed.

At the same time, there is a pressing need to evaluate the strength of emotional expressions beyond binary categories, diversifying the classification of emotional intensities. In our study, we aimed to map emotional intensities from human speech signals across four categories: Neutral, Onset, Offset, and Apex.

Our source code for system implementation is included in this repository. We encourage researchers and engineers interested in this field to explore our published work for more details. If our research or implementation aids your work, we kindly request you to cite it.

DOI: 10.1109/ICIIS58898.2023.10253609
Cite this:
H. Rajendran, H. M. R. T. Bandara, D. P. Chandima and A. G. B. P. Jayasekara, "Voice Response Based Emotion Intensity Classification for Assistive Robots," 2023 IEEE 17th International Conference on Industrial and Information Systems (ICIIS), Peradeniya, Sri Lanka, 2023, pp. 1-6, doi: 10.1109/ICIIS58898.2023.10253609. keywords: {Training;Emotion recognition;Machine learning algorithms;System performance;Machine learning;Feature extraction;Prediction algorithms;Human-Robot Interaction;Emotion intensity;Emotion classification}.
